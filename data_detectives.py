# -*- coding: utf-8 -*-
"""Data Detectives

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p9sRmMeLZch6sKiPVcDjsVkifzsynHEV
"""

import pandas as pd
import numpy as np
import os
import cv2
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import Dense, Flatten, Input, Concatenate, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Load Data
train_df = pd.read_csv('train.csv')
category_attributes = pd.read_parquet('category_attributes.parquet')

# Helper: Load images and resize them
def load_and_resize_image(image_id, image_dir, target_size=(224, 224)):
    img_path = os.path.join(image_dir, f"{image_id}.jpg")
    img = cv2.imread(img_path)
    if img is not None:
        img = cv2.resize(img, target_size)
        # Normalize
        img = img / 255.0
    else:
        # Fallback for missing images
        img = np.zeros((*target_size, 3))
    return img

# Load and preprocess images
image_dir = 'images_path/'
train_df['image_data'] = train_df['id'].apply(lambda x: load_and_resize_image(x, image_dir))
X_images = np.stack(train_df['image_data'])

# Preprocess text attributes
label_encoders = {}
for col in [f'attr_{i}' for i in range(1, 11)]:
    le = LabelEncoder()
    train_df[col] = le.fit_transform(train_df[col].fillna('dummy_value'))
    label_encoders[col] = le

# Convert to categorical
y = [to_categorical(train_df[f'attr_{i}']) for i in range(1, 11)]

# Train/Test Split
X_train_images, X_val_images, y_train, y_val = train_test_split(
    X_images, list(zip(*y)), test_size=0.2, random_state=42
)

# Build the Model
def build_model(input_shape=(224, 224, 3), num_classes_list=None):
    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')
    x = Flatten()(base_model.output)
    x = Dropout(0.3)(x)
    outputs = [Dense(num_classes, activation='softmax', name=f'attr_{i+1}')(x) for i, num_classes in enumerate(num_classes_list)]
    model = Model(inputs=base_model.input, outputs=outputs)
    return model

# Number of unique classes for each attribute
num_classes_list = [len(le.classes_) for le in label_encoders.values()]

# Compile the Model
model = build_model(num_classes_list=num_classes_list)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the Model
history = model.fit(
    X_train_images, y_train,
    validation_data=(X_val_images, y_val),
    batch_size=32, epochs=10
)

# Generate Predictions for Test Set
test_df = pd.read_csv('test.csv')
test_df['image_data'] = test_df['id'].apply(lambda x: load_and_resize_image(x, image_dir))
X_test_images = np.stack(test_df['image_data'])

# Predict
predictions = model.predict(X_test_images)
predictions_decoded = [
    [label_encoders[f'attr_{i+1}'].inverse_transform(np.argmax(p, axis=1)) for i, p in enumerate(predictions)]
]

# Create Submission
submission = test_df[['id', 'Category']].copy()
for i in range(1, 11):
    submission[f'attr_{i}'] = predictions_decoded[i-1]
submission.to_csv('submission.csv', index=False)

